{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insider Threat Detection - Data Exploration\n",
    "\n",
    "This notebook provides an interactive exploration of the insider threat detection dataset and demonstrates the key features of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Import project modules\n",
    "from src.data.loader import DataLoader\n",
    "from src.data.preprocessor import DataPreprocessor\n",
    "from src.data.feature_engineer import FeatureEngineer\n",
    "from src.evaluation.visualizer import ModelVisualizer\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Basic Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Load and merge data\n",
    "try:\n",
    "    merged_df = data_loader.load_and_merge_data()\n",
    "    print(f\"✅ Data loaded successfully: {len(merged_df)} records\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "    print(\"Please ensure your data files are in the correct location.\")\n",
    "    merged_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merged_df is not None:\n",
    "    # Basic data info\n",
    "    print(\"Dataset Overview:\")\n",
    "    print(f\"Shape: {merged_df.shape}\")\n",
    "    print(f\"Columns: {list(merged_df.columns)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(merged_df.head())\n",
    "    \n",
    "    print(\"\\nData types:\")\n",
    "    display(merged_df.dtypes)\n",
    "    \n",
    "    print(\"\\nMissing values:\")\n",
    "    display(merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merged_df is not None:\n",
    "    # Get data summary\n",
    "    summary = data_loader.get_data_summary()\n",
    "    \n",
    "    print(\"Data Summary:\")\n",
    "    for key, value in summary.items():\n",
    "        if key != 'activity_types':\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nActivity Types Distribution:\")\n",
    "    for activity, count in summary['activity_types'].items():\n",
    "        print(f\"  {activity}: {count} ({count/summary['total_records']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merged_df is not None:\n",
    "    # Validate data quality\n",
    "    validation_results = data_loader.validate_data_quality()\n",
    "    \n",
    "    print(f\"Data Quality Validation: {'✅ PASSED' if validation_results['is_valid'] else '❌ ISSUES FOUND'}\")\n",
    "    \n",
    "    if validation_results['issues']:\n",
    "        print(\"\\nIssues found:\")\n",
    "        for issue in validation_results['issues']:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(\"No data quality issues detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merged_df is not None:\n",
    "    # Activity type distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    activity_counts = merged_df['activity_type'].value_counts()\n",
    "    plt.pie(activity_counts.values, labels=activity_counts.index, autopct='%1.1f%%')\n",
    "    plt.title('Activity Type Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    activity_counts.plot(kind='bar')\n",
    "    plt.title('Activity Type Counts')\n",
    "    plt.xlabel('Activity Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merged_df is not None:\n",
    "    # User activity analysis\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Top users by activity count\n",
    "    plt.subplot(2, 2, 1)\n",
    "    top_users = merged_df['user'].value_counts().head(10)\n",
    "    top_users.plot(kind='bar')\n",
    "    plt.title('Top 10 Most Active Users')\n",
    "    plt.xlabel('User')\n",
    "    plt.ylabel('Activity Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # PC usage distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    top_pcs = merged_df['pc'].value_counts().head(10)\n",
    "    top_pcs.plot(kind='bar')\n",
    "    plt.title('Top 10 Most Used PCs')\n",
    "    plt.xlabel('PC')\n",
    "    plt.ylabel('Usage Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Unique users and PCs\n",
    "    plt.subplot(2, 2, 3)\n",
    "    unique_counts = {\n",
    "        'Users': merged_df['user'].nunique(),\n",
    "        'PCs': merged_df['pc'].nunique(),\n",
    "        'Activities': len(merged_df)\n",
    "    }\n",
    "    plt.bar(unique_counts.keys(), unique_counts.values())\n",
    "    plt.title('Dataset Statistics')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Activity distribution by user\n",
    "    plt.subplot(2, 2, 4)\n",
    "    user_activity_counts = merged_df.groupby('user').size()\n",
    "    plt.hist(user_activity_counts, bins=20, alpha=0.7)\n",
    "    plt.title('Distribution of Activities per User')\n",
    "    plt.xlabel('Number of Activities')\n",
    "    plt.ylabel('Number of Users')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merged_df is not None:\n",
    "    # Initialize preprocessor and feature engineer\n",
    "    preprocessor = DataPreprocessor()\n",
    "    feature_engineer = FeatureEngineer()\n",
    "    \n",
    "    # Create a copy for processing\n",
    "    df_processed = merged_df.copy()\n",
    "    \n",
    "    try:\n",
    "        # Basic preprocessing\n",
    "        df_processed = preprocessor.clean_dates(df_processed)\n",
    "        df_processed = preprocessor.create_time_features(df_processed)\n",
    "        df_processed = preprocessor.handle_missing_values(df_processed)\n",
    "        \n",
    "        print(\"✅ Basic preprocessing completed\")\n",
    "        print(f\"New features added: {set(df_processed.columns) - set(merged_df.columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in preprocessing: {e}\")\n",
    "        df_processed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_processed is not None:\n",
    "    # Time-based analysis\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Hourly activity distribution\n",
    "    plt.subplot(2, 3, 1)\n",
    "    hourly_activity = df_processed['hour'].value_counts().sort_index()\n",
    "    hourly_activity.plot(kind='bar')\n",
    "    plt.title('Activity Distribution by Hour')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Activity Count')\n",
    "    \n",
    "    # Day of week distribution\n",
    "    plt.subplot(2, 3, 2)\n",
    "    day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    daily_activity = df_processed['day_of_week'].value_counts().sort_index()\n",
    "    plt.bar(range(len(daily_activity)), daily_activity.values)\n",
    "    plt.xticks(range(len(day_names)), day_names)\n",
    "    plt.title('Activity Distribution by Day of Week')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Activity Count')\n",
    "    \n",
    "    # Weekend vs weekday\n",
    "    plt.subplot(2, 3, 3)\n",
    "    weekend_counts = df_processed['is_weekend'].value_counts()\n",
    "    labels = ['Weekday', 'Weekend']\n",
    "    plt.pie(weekend_counts.values, labels=labels, autopct='%1.1f%%')\n",
    "    plt.title('Weekend vs Weekday Activity')\n",
    "    \n",
    "    # Off-hours activity\n",
    "    plt.subplot(2, 3, 4)\n",
    "    offhours_counts = df_processed['is_off_hours'].value_counts()\n",
    "    labels = ['Business Hours', 'Off Hours']\n",
    "    plt.pie(offhours_counts.values, labels=labels, autopct='%1.1f%%')\n",
    "    plt.title('Business Hours vs Off Hours')\n",
    "    \n",
    "    # Cyclical time features\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.scatter(df_processed['hour_cos'], df_processed['hour_sin'], alpha=0.5)\n",
    "    plt.title('Cyclical Hour Representation')\n",
    "    plt.xlabel('Hour Cosine')\n",
    "    plt.ylabel('Hour Sine')\n",
    "    \n",
    "    # Activity timeline (sample)\n",
    "    plt.subplot(2, 3, 6)\n",
    "    if len(df_processed) > 1000:\n",
    "        sample_df = df_processed.sample(1000).sort_values('date')\n",
    "    else:\n",
    "        sample_df = df_processed.sort_values('date')\n",
    "    \n",
    "    plt.plot(sample_df['date'], range(len(sample_df)), alpha=0.7)\n",
    "    plt.title('Activity Timeline (Sample)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Activity Index')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_processed is not None:\n",
    "    try:\n",
    "        # Advanced feature engineering\n",
    "        df_processed = preprocessor.sort_and_prepare(df_processed)\n",
    "        df_processed = feature_engineer.create_user_behavior_features(df_processed)\n",
    "        df_processed = feature_engineer.encode_categorical_features(df_processed)\n",
    "        df_processed = feature_engineer.detect_anomalies(df_processed)\n",
    "        df_processed = feature_engineer.create_threat_labels(df_processed)\n",
    "        \n",
    "        print(\"✅ Advanced feature engineering completed\")\n",
    "        print(f\"Total features: {len(df_processed.columns)}\")\n",
    "        print(f\"Threat ratio: {df_processed['is_threat'].mean():.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in advanced feature engineering: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_processed is not None and 'is_threat' in df_processed.columns:\n",
    "    # Threat analysis\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Threat distribution\n",
    "    plt.subplot(2, 3, 1)\n",
    "    threat_counts = df_processed['is_threat'].value_counts()\n",
    "    labels = ['Normal', 'Threat']\n",
    "    plt.pie(threat_counts.values, labels=labels, autopct='%1.1f%%')\n",
    "    plt.title('Threat vs Normal Distribution')\n",
    "    \n",
    "    # Anomaly distribution\n",
    "    plt.subplot(2, 3, 2)\n",
    "    anomaly_counts = df_processed['anomaly_score'].value_counts()\n",
    "    labels = ['Normal', 'Anomaly']\n",
    "    plt.pie(anomaly_counts.values, labels=labels, autopct='%1.1f%%')\n",
    "    plt.title('Anomaly Distribution')\n",
    "    \n",
    "    # User activity entropy distribution\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.hist(df_processed['activity_entropy'], bins=20, alpha=0.7)\n",
    "    plt.title('Activity Entropy Distribution')\n",
    "    plt.xlabel('Entropy')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Off-hours ratio by threat\n",
    "    plt.subplot(2, 3, 4)\n",
    "    threat_offhours = df_processed.groupby('is_threat')['off_hours_ratio'].mean()\n",
    "    threat_offhours.plot(kind='bar')\n",
    "    plt.title('Average Off-Hours Ratio by Threat Status')\n",
    "    plt.xlabel('Threat Status')\n",
    "    plt.ylabel('Off-Hours Ratio')\n",
    "    plt.xticks([0, 1], ['Normal', 'Threat'], rotation=0)\n",
    "    \n",
    "    # Unique PCs by threat\n",
    "    plt.subplot(2, 3, 5)\n",
    "    threat_pcs = df_processed.groupby('is_threat')['unique_pcs'].mean()\n",
    "    threat_pcs.plot(kind='bar')\n",
    "    plt.title('Average Unique PCs by Threat Status')\n",
    "    plt.xlabel('Threat Status')\n",
    "    plt.ylabel('Unique PCs')\n",
    "    plt.xticks([0, 1], ['Normal', 'Threat'], rotation=0)\n",
    "    \n",
    "    # Feature correlation with threat\n",
    "    plt.subplot(2, 3, 6)\n",
    "    numeric_features = ['off_hours_ratio', 'weekend_ratio', 'activity_entropy', \n",
    "                       'unique_pcs', 'anomaly_score']\n",
    "    available_features = [f for f in numeric_features if f in df_processed.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        correlations = df_processed[available_features + ['is_threat']].corr()['is_threat'][:-1]\n",
    "        correlations.plot(kind='bar')\n",
    "        plt.title('Feature Correlation with Threat')\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Correlation')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_processed is not None:\n",
    "    print(\"📊 Data Exploration Summary\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total Records: {len(df_processed):,}\")\n",
    "    print(f\"Total Features: {len(df_processed.columns)}\")\n",
    "    print(f\"Unique Users: {df_processed['user'].nunique():,}\")\n",
    "    print(f\"Unique PCs: {df_processed['pc'].nunique():,}\")\n",
    "    print(f\"Date Range: {df_processed['date'].min()} to {df_processed['date'].max()}\")\n",
    "    \n",
    "    if 'is_threat' in df_processed.columns:\n",
    "        print(f\"Threat Ratio: {df_processed['is_threat'].mean():.1%}\")\n",
    "        print(f\"Anomaly Ratio: {df_processed['anomaly_score'].mean():.1%}\")\n",
    "    \n",
    "    print(\"\\n🚀 Next Steps:\")\n",
    "    print(\"1. Run model training: python main.py train\")\n",
    "    print(\"2. Evaluate model performance: python main.py evaluate --model-path path/to/model.h5\")\n",
    "    print(\"3. Make predictions: python main.py predict --model-path path/to/model.h5 --input-file data.csv\")\n",
    "    print(\"4. Try the complete demo: python main.py demo\")\n",
    "else:\n",
    "    print(\"❌ Data exploration could not be completed due to data loading issues.\")\n",
    "    print(\"Please check your data files and try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
